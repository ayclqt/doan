{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3351831",
   "metadata": {
    "id": "c3351831"
   },
   "source": [
    "# Qdrant Database Import/Export with LangChain\n",
    "\n",
    "Notebook này hướng dẫn cách sử dụng LangChain và Qdrant trên Google Colab, sau đó export vector database để sử dụng ở local."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317d7cd",
   "metadata": {
    "id": "e317d7cd"
   },
   "source": [
    "## Phần 1: Cài đặt trên Colab\n",
    "\n",
    "Đầu tiên, chúng ta cần cài đặt các thư viện cần thiết trên Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022577a6",
   "metadata": {
    "id": "022577a6"
   },
   "outputs": [],
   "source": [
    "# Cài đặt các thư viện cần thiết\n",
    "!pip install langchain langchain-openai langchain-community langchain-huggingface langchain-qdrant\\\n",
    "    qdrant-client python-dotenv pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf3576",
   "metadata": {
    "id": "9ddf3576"
   },
   "source": [
    "## Phần 2: Upload Dữ liệu Sản phẩm\n",
    "\n",
    "Chúng ta cần upload file dữ liệu sản phẩm lên Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba3bc3",
   "metadata": {
    "id": "29ba3bc3"
   },
   "outputs": [],
   "source": [
    "# Sử dụng widget upload file của Colab\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Lấy tên file đầu tiên được upload\n",
    "data_file = list(uploaded.keys())[0]\n",
    "print(f\"Đã upload file: {data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c8bccd",
   "metadata": {
    "id": "76c8bccd"
   },
   "source": [
    "## Phần 3: Thiết lập Qdrant trong Colab\n",
    "\n",
    "Chúng ta sẽ sử dụng Qdrant locally trong Colab (không cần Docker):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05339c6e",
   "metadata": {
    "id": "05339c6e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "# Khởi tạo Qdrant local trong Colab (sẽ lưu vào RAM)\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Thiết lập các biến môi trường\n",
    "\n",
    "\n",
    "# Thiết lập OpenAI API key (thay thế với API key của bạn)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"  # Thay thế bằng key của bạn\n",
    "\n",
    "# Các biến cấu hình\n",
    "COLLECTION_NAME = \"product_data\"\n",
    "EMBEDDING_MODEL_NAME = \"AITeamVN/Vietnamese_Embedding\"\n",
    "VECTOR_SIZE = 1024  # Kích thước vector cho mô hình AITeamVN/Vietnamese_Embedding\n",
    "\n",
    "# Tạo collection\n",
    "if not qdrant_client.collection_exists(COLLECTION_NAME):\n",
    "    qdrant_client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "    print(f\"Đã tạo collection '{COLLECTION_NAME}' trong Qdrant\")\n",
    "else:\n",
    "    print(\n",
    "        f\"Đã tồn tại collection '{COLLECTION_NAME}' trong Qdrant, sẽ sử dụng lại hoặc tạo lại nếu cần.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafaec6b",
   "metadata": {
    "id": "eafaec6b"
   },
   "source": [
    "## Phần 4: Triển khai code LangChain và Text Processor\n",
    "\n",
    "Chúng ta sẽ port code từ project vào đây:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e9ebd",
   "metadata": {
    "id": "292e9ebd"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Cấu hình chunking\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "\n",
    "# Text Processor class\n",
    "class TextProcessor:\n",
    "    \"\"\"Process text data for embedding.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, chunk_size: int = CHUNK_SIZE, chunk_overlap: int = CHUNK_OVERLAP\n",
    "    ):\n",
    "        \"\"\"Initialize text processor with chunking parameters.\"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        )\n",
    "\n",
    "    def load_data(self, file_path: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Load product data from JSON file.\"\"\"\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                print(f\"Loaded {len(data)} products from {file_path}\")\n",
    "                return data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading data: {e}\")\n",
    "            return []\n",
    "\n",
    "    def product_to_text(self, product: Dict[str, Any]) -> str:\n",
    "        \"\"\"Convert a product dictionary to formatted text.\"\"\"\n",
    "        product_name = product.get(\"Tên\", \"Unknown Product\")\n",
    "        product_text = [f\"Tên sản phẩm: {product_name}\"]\n",
    "\n",
    "        # Add all other properties\n",
    "        for key, value in product.items():\n",
    "            if key != \"Tên\":  # Skip name as we already included it\n",
    "                product_text.append(f\"{key}: {value}\")\n",
    "\n",
    "        return \"\\n\".join(product_text)\n",
    "\n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks for embedding.\"\"\"\n",
    "        chunks = self.text_splitter.split_text(text)\n",
    "        return chunks\n",
    "\n",
    "    def chunk_product(self, product: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a single product into chunks with metadata.\"\"\"\n",
    "        product_text = self.product_to_text(product)\n",
    "\n",
    "        # For products, we might want smaller chunks if there's a lot of info\n",
    "        chunks = self.chunk_text(product_text)\n",
    "        product_chunks = []\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Create a chunk document with metadata\n",
    "            product_chunks.append(\n",
    "                {\n",
    "                    \"text\": chunk,\n",
    "                    \"metadata\": {\n",
    "                        \"product_id\": product.get(\"id\", i),\n",
    "                        \"product_name\": product.get(\"Tên\", \"Unknown\"),\n",
    "                        \"chunk_id\": i,\n",
    "                        \"total_chunks\": len(chunks),\n",
    "                        **product,  # Include all product data in metadata\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return product_chunks\n",
    "\n",
    "    def process_all_products(\n",
    "        self, products: List[Dict[str, Any]]\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process all products into chunks ready for embedding.\"\"\"\n",
    "        all_chunks = []\n",
    "\n",
    "        for i, product in enumerate(products):\n",
    "            # Add an ID if not present\n",
    "            if \"id\" not in product:\n",
    "                product[\"id\"] = i\n",
    "\n",
    "            product_chunks = self.chunk_product(product)\n",
    "            all_chunks.extend(product_chunks)\n",
    "\n",
    "        print(f\"Created {len(all_chunks)} chunks from {len(products)} products\")\n",
    "        return all_chunks\n",
    "\n",
    "\n",
    "# VectorStore class\n",
    "class VectorStore:\n",
    "    \"\"\"Manages vector database operations with Qdrant.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        client=None,\n",
    "        collection_name: str = COLLECTION_NAME,\n",
    "        embedding_model: str = EMBEDDING_MODEL_NAME,\n",
    "    ):\n",
    "        \"\"\"Initialize the vector database connection.\"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model,\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            multi_process=True,\n",
    "        )\n",
    "\n",
    "        # Use provided client or create in-memory client\n",
    "        self.client = client or QdrantClient(\":memory:\")\n",
    "\n",
    "        # Initialize Langchain's Qdrant wrapper\n",
    "        self.vectorstore = None\n",
    "\n",
    "    def initialize_vectorstore(self, force_recreate: bool = False) -> QdrantVectorStore:\n",
    "        \"\"\"Initialize the vector store for LangChain operations.\"\"\"\n",
    "        self.vectorstore = QdrantVectorStore(\n",
    "            client=self.client,\n",
    "            collection_name=self.collection_name,\n",
    "            embedding=self.embedding_model,\n",
    "        )\n",
    "        return self.vectorstore\n",
    "\n",
    "    def get_vectorstore(self) -> QdrantVectorStore:\n",
    "        \"\"\"Get the initialized vector store.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return self.initialize_vectorstore()\n",
    "        return self.vectorstore\n",
    "\n",
    "    def prepare_documents(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Convert raw data to document format for embedding and storage.\"\"\"\n",
    "        documents = []\n",
    "\n",
    "        for i, product in enumerate(data):\n",
    "            # Extract product text and metadata\n",
    "            text = product.get(\"text\", \"\")\n",
    "            metadata = product.get(\"metadata\", {})\n",
    "\n",
    "            # Create document with metadata\n",
    "            document = {\n",
    "                \"product_id\": metadata.get(\"product_id\", i),\n",
    "                \"product_name\": metadata.get(\"product_name\", \"Unknown\"),\n",
    "                \"text\": text,\n",
    "                \"metadata\": metadata,\n",
    "            }\n",
    "            documents.append(document)\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def index_documents(self, documents: List[Dict[str, Any]]) -> None:\n",
    "        \"\"\"Index documents into the vector database.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            self.initialize_vectorstore(force_recreate=True)  # Add force_recreate here\n",
    "\n",
    "        # Extract text and metadata\n",
    "        texts = [doc[\"text\"] for doc in documents]\n",
    "        metadatas = [doc[\"metadata\"] for doc in documents]\n",
    "\n",
    "        # Add texts to the vector store\n",
    "        self.vectorstore.add_texts(texts=texts, metadatas=metadatas)\n",
    "        print(f\"Indexed {len(documents)} documents into Qdrant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9414c297",
   "metadata": {
    "id": "9414c297"
   },
   "source": [
    "## Phần 5: Nạp dữ liệu vào Qdrant\n",
    "\n",
    "Bây giờ chúng ta sẽ nạp dữ liệu JSON vào Qdrant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26110d",
   "metadata": {
    "id": "ef26110d"
   },
   "outputs": [],
   "source": [
    "# Nạp và xử lý dữ liệu\n",
    "text_processor = TextProcessor()\n",
    "vector_store = VectorStore(client=qdrant_client)\n",
    "\n",
    "# Load data\n",
    "raw_data = text_processor.load_data(data_file)\n",
    "\n",
    "# Process data\n",
    "processed_chunks = text_processor.process_all_products(raw_data)\n",
    "\n",
    "# Prepare documents\n",
    "documents = vector_store.prepare_documents(processed_chunks)\n",
    "\n",
    "# Index documents\n",
    "vector_store.index_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e94f18",
   "metadata": {
    "id": "44e94f18"
   },
   "source": [
    "## Phần 6: Export Qdrant Collection\n",
    "\n",
    "Bây giờ chúng ta sẽ export collection từ Qdrant để có thể import vào local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b922f",
   "metadata": {
    "id": "5e9b922f"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def export_qdrant_collection(client, collection_name, output_file=\"qdrant_export.pkl\"):\n",
    "    \"\"\"Export a Qdrant collection to a portable file format.\"\"\"\n",
    "    # Get collection info\n",
    "    collection_info = client.get_collection(collection_name=collection_name)\n",
    "\n",
    "    # Get all points with their vectors and payloads\n",
    "    # We'll retrieve in batches to handle large collections\n",
    "    limit = 1000\n",
    "    offset = 0\n",
    "    all_points = []\n",
    "\n",
    "    while True:\n",
    "        points = client.scroll(\n",
    "            collection_name=collection_name,\n",
    "            limit=limit,\n",
    "            offset=offset,\n",
    "            with_vectors=True,\n",
    "            with_payload=True,\n",
    "        )[0]\n",
    "\n",
    "        if not points:\n",
    "            break\n",
    "\n",
    "        all_points.extend(points)\n",
    "        offset += limit\n",
    "\n",
    "        print(f\"Retrieved {len(all_points)} points so far...\")\n",
    "\n",
    "        if len(points) < limit:\n",
    "            break\n",
    "\n",
    "    # Create the export data structure\n",
    "    export_data = {\n",
    "        \"collection_info\": {\n",
    "            \"name\": collection_name,\n",
    "            \"vector_size\": collection_info.config.params.vectors.size,\n",
    "            \"vector_distance\": collection_info.config.params.vectors.distance.name,\n",
    "        },\n",
    "        \"points\": [\n",
    "            {\"id\": point.id, \"vector\": point.vector, \"payload\": point.payload}\n",
    "            for point in all_points\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    # Save to file\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        pickle.dump(export_data, f)\n",
    "\n",
    "    print(f\"Exported {len(all_points)} points to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "# Export collection\n",
    "export_file = export_qdrant_collection(qdrant_client, COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e32d1",
   "metadata": {
    "id": "d69e32d1"
   },
   "source": [
    "## Phần 7: Download Export File\n",
    "\n",
    "Tải xuống file export để sử dụng trong môi trường local:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e8ca2",
   "metadata": {
    "id": "840e8ca2"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download(\"qdrant_export.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d3652a",
   "metadata": {
    "id": "87d3652a"
   },
   "source": [
    "## Phần 8: Test Vector Search trên Colab\n",
    "\n",
    "Kiểm tra xem vector search có hoạt động chính xác không:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae566f73",
   "metadata": {
    "id": "ae566f73"
   },
   "outputs": [],
   "source": [
    "# Test truy vấn\n",
    "vectorstore = vector_store.get_vectorstore()\n",
    "results = vectorstore.similarity_search_with_score(\"điện thoại có camera tốt nhất\", k=3)\n",
    "\n",
    "# Display results\n",
    "for i, (doc, score) in enumerate(results):\n",
    "    print(f\"\\nKết quả #{i + 1} (score: {score:.4f})\")\n",
    "    print(f\"Sản phẩm: {doc.metadata.get('product_name', 'Unknown')}\")\n",
    "    print(f\"Nội dung: {doc.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be923dd8",
   "metadata": {
    "id": "be923dd8"
   },
   "source": [
    "# Hướng dẫn Import dữ liệu trên Local\n",
    "\n",
    "Sau khi bạn đã export và download file `qdrant_export.pkl`, dưới đây là các bước để import nó vào môi trường local. Hãy tạo file Python mới trên local với nội dung sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e380384",
   "metadata": {
    "id": "3e380384"
   },
   "outputs": [],
   "source": [
    "# Code này sẽ chạy trên máy local của bạn, không phải trên Colab\n",
    "# Lưu nội dung này vào file import_qdrant.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "\n",
    "\n",
    "def import_to_qdrant(export_file, host=\"localhost\", port=6333):\n",
    "    \"\"\"Import a previously exported Qdrant collection.\"\"\"\n",
    "    print(f\"Connecting to Qdrant at {host}:{port}...\")\n",
    "    client = QdrantClient(host=host, port=port)\n",
    "\n",
    "    # Load the export data\n",
    "    with open(export_file, \"rb\") as f:\n",
    "        export_data = pickle.load(f)\n",
    "\n",
    "    collection_info = export_data[\"collection_info\"]\n",
    "    points = export_data[\"points\"]\n",
    "\n",
    "    collection_name = collection_info[\"name\"]\n",
    "    vector_size = collection_info[\"vector_size\"]\n",
    "    distance_str = collection_info[\"vector_distance\"]\n",
    "\n",
    "    # Map string distance to enum\n",
    "    distance_map = {\n",
    "        \"COSINE\": Distance.COSINE,\n",
    "        \"EUCLID\": Distance.EUCLID,\n",
    "        \"DOT\": Distance.DOT,\n",
    "    }\n",
    "    distance = distance_map.get(distance_str, Distance.COSINE)\n",
    "\n",
    "    # Check if collection exists and recreate it\n",
    "    collections = client.get_collections().collections\n",
    "    collection_names = [c.name for c in collections]\n",
    "\n",
    "    if collection_name in collection_names:\n",
    "        print(f\"Collection '{collection_name}' already exists. Recreating...\")\n",
    "        client.delete_collection(collection_name=collection_name)\n",
    "\n",
    "    # Create collection\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=vector_size, distance=distance),\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Created collection '{collection_name}' with {vector_size} vector dimensions\"\n",
    "    )\n",
    "\n",
    "    # Import points in batches\n",
    "    batch_size = 100\n",
    "    total_points = len(points)\n",
    "\n",
    "    for i in range(0, total_points, batch_size):\n",
    "        batch = points[i : i + batch_size]\n",
    "\n",
    "        # Format points for upsert\n",
    "        upsert_points = [\n",
    "            {\"id\": point[\"id\"], \"vector\": point[\"vector\"], \"payload\": point[\"payload\"]}\n",
    "            for point in batch\n",
    "        ]\n",
    "\n",
    "        # Upsert points\n",
    "        client.upsert(collection_name=collection_name, points=upsert_points)\n",
    "\n",
    "        print(f\"Imported {min(i + batch_size, total_points)}/{total_points} points\")\n",
    "\n",
    "    # Verify import\n",
    "    count = client.count(collection_name=collection_name).count\n",
    "    print(f\"Import complete. Collection '{collection_name}' now has {count} points.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Import Qdrant collection from export file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"export_file\", help=\"Path to the export file (qdrant_export.pkl)\"\n",
    "    )\n",
    "    parser.add_argument(\"--host\", default=\"localhost\", help=\"Qdrant host\")\n",
    "    parser.add_argument(\"--port\", type=int, default=6333, help=\"Qdrant port\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    import_to_qdrant(args.export_file, args.host, args.port)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47212d3f",
   "metadata": {
    "id": "47212d3f"
   },
   "source": [
    "## Thực hiện Import trên Local\n",
    "\n",
    "Sau khi bạn đã tạo script `import_qdrant.py` trên local và tải file `qdrant_export.pkl` từ Colab, chạy lệnh sau trên terminal của bạn:\n",
    "\n",
    "```bash\n",
    "python import_qdrant.py qdrant_export.pkl --host localhost --port 6333\n",
    "```\n",
    "\n",
    "Sau đó, bạn có thể sử dụng collection đã import với code LangChain của mình."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}