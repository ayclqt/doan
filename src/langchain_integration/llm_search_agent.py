"""
LLM-driven search decision system for intelligent query routing and search strategy.

This module implements an agent-based approach where the LLM makes decisions about:
1. Whether to perform web search
2. What search queries to use
3. How to combine multiple search results
4. When to perform follow-up searches

Author: Assistant
"""

import json
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

from ..config import config, logger
from .web_search import SearchResult, WebSearcher

__author__ = "Assistant"
__copyright__ = "Copyright 2025"
__maintainer__ = "Assistant"
__status__ = "Development"


@dataclass
class SearchDecision:
    """Decision made by LLM about search strategy."""

    should_search: bool
    search_queries: List[str]
    reasoning: str
    confidence: float
    search_type: str  # "web", "vector", "hybrid", "none"


@dataclass
class SearchPlan:
    """Complete search plan generated by LLM."""

    primary_decision: SearchDecision
    fallback_strategies: List[SearchDecision]
    expected_information: List[str]
    success_criteria: str


class LLMSearchAgent:
    """LLM-powered search decision agent."""

    def __init__(
        self,
        model_name: str = None,
        temperature: float = 0.1,  # Low temperature for consistent decisions
        web_searcher: Optional[WebSearcher] = None,
    ):
        """Initialize the LLM search agent."""
        self.llm = ChatOpenAI(
            model=model_name or config.llm_model_name,
            temperature=temperature,
            max_tokens=1000,  # Decisions don't need long responses
            api_key=config.openai_api_key,
            base_url=config.openai_base_url,
        )

        self.web_searcher = web_searcher or WebSearcher()
        self.logger = logger

        # Decision prompt template
        self.decision_prompt = self._create_decision_prompt()
        self.query_generation_prompt = self._create_query_generation_prompt()

        # Decision chain
        self.decision_chain = self.decision_prompt | self.llm | StrOutputParser()
        self.query_chain = self.query_generation_prompt | self.llm | StrOutputParser()

    def _create_decision_prompt(self) -> ChatPromptTemplate:
        """Create prompt template for search decisions."""
        system_template = """
        Bạn là một AI agent chuyên về quyết định chiến lược tìm kiếm thông tin.

        Nhiệm vụ: Phân tích câu hỏi và ngữ cảnh để quyết định có nên tìm kiếm web không.

        Hướng dẫn quyết định:
        1. SỬ DỤNG WEB SEARCH khi:
           - Câu hỏi về giá cả, khuyến mãi (thông tin thay đổi thường xuyên)
           - So sánh sản phẩm cần thông tin cập nhật
           - Câu hỏi về sản phẩm mới, xu hướng công nghệ
           - Vector search results không đủ hoặc không liên quan
           - Cần thông tin bổ sung từ nhiều nguồn

        2. KHÔNG SỬ DỤNG WEB SEARCH khi:
           - Vector search results đã đủ chi tiết và chính xác
           - Câu hỏi về thông số kỹ thuật cơ bản (ít thay đổi)
           - Thông tin trong database đã đầy đủ

        Vector Search Results: {vector_results}
        Câu hỏi: {question}
        Lịch sử cuộc trò chuyện: {conversation_context}

        Trả lời theo định dạng JSON:
        {{
            "should_search": true/false,
            "reasoning": "lý do quyết định",
            "confidence": 0.0-1.0,
            "search_type": "web|vector|hybrid|none",
            "expected_info": ["thông tin cần tìm 1", "thông tin cần tìm 2"]
        }}
        """

        return ChatPromptTemplate.from_messages(
            [
                ("system", system_template),
                ("human", "Hãy quyết định chiến lược tìm kiếm cho câu hỏi này."),
            ]
        )

    def _create_query_generation_prompt(self) -> ChatPromptTemplate:
        """Create prompt template for search query generation."""
        system_template = """
        Bạn là chuyên gia tạo search query cho tìm kiếm sản phẩm điện tử.

        Nhiệm vụ: Tạo các search query tối ưu dựa trên câu hỏi và ngữ cảnh.

        Nguyên tắc tạo query:
        1. Query ngắn gọn, chính xác (5-10 từ)
        2. Bao gồm tên sản phẩm cụ thể
        3. Loại bỏ từ tham chiếu ("điện thoại trên", "sản phẩm đó")
        4. Thêm từ khóa "điện tử" nếu cần
        5. Ưu tiên tiếng Việt

        Câu hỏi gốc: {original_question}
        Ngữ cảnh cuộc trò chuyện: {conversation_context}
        Thông tin cần tìm: {expected_info}

        Trả lời theo định dạng JSON:
        {{
            "primary_query": "query chính",
            "alternative_queries": ["query phụ 1", "query phụ 2"],
            "query_explanation": "giải thích logic tạo query"
        }}
        """

        return ChatPromptTemplate.from_messages(
            [
                ("system", system_template),
                ("human", "Tạo search queries tối ưu cho yêu cầu này."),
            ]
        )

    def decide_search_strategy(
        self,
        question: str,
        vector_results: List[Any],
        conversation_history: Optional[List[dict]] = None,
    ) -> SearchDecision:
        """Let LLM decide search strategy."""
        try:
            # Prepare context
            vector_summary = self._summarize_vector_results(vector_results)
            conversation_context = self._format_conversation_context(
                conversation_history
            )

            # Get LLM decision
            decision_input = {
                "question": question,
                "vector_results": vector_summary,
                "conversation_context": conversation_context,
            }

            decision_response = self.decision_chain.invoke(decision_input)

            # Parse LLM response
            try:
                decision_data = json.loads(decision_response)

                return SearchDecision(
                    should_search=decision_data.get("should_search", False),
                    search_queries=[],  # Will be generated separately if needed
                    reasoning=decision_data.get("reasoning", ""),
                    confidence=decision_data.get("confidence", 0.5),
                    search_type=decision_data.get("search_type", "none"),
                )

            except json.JSONDecodeError:
                # Fallback parsing if JSON fails
                should_search = "true" in decision_response.lower()
                self.logger.warning(
                    f"Failed to parse LLM decision JSON, fallback: {should_search}"
                )

                return SearchDecision(
                    should_search=should_search,
                    search_queries=[],
                    reasoning="JSON parse failed, using fallback logic",
                    confidence=0.3,
                    search_type="web" if should_search else "none",
                )

        except Exception as e:
            self.logger.error(f"Error in search decision: {e}")
            # Fallback to rule-based decision
            return self._fallback_decision(question, vector_results)

    def generate_search_queries(
        self,
        question: str,
        expected_info: List[str],
        conversation_history: Optional[List[dict]] = None,
    ) -> List[str]:
        """Generate optimized search queries using LLM."""
        try:
            conversation_context = self._format_conversation_context(
                conversation_history
            )

            query_input = {
                "original_question": question,
                "expected_info": ", ".join(expected_info),
                "conversation_context": conversation_context,
            }

            query_response = self.query_chain.invoke(query_input)

            try:
                query_data = json.loads(query_response)

                queries = [query_data.get("primary_query", question)]
                queries.extend(query_data.get("alternative_queries", []))

                # Filter out empty queries and duplicates
                queries = list(set([q.strip() for q in queries if q and q.strip()]))

                self.logger.info(f"Generated {len(queries)} search queries: {queries}")
                return queries

            except json.JSONDecodeError:
                # Fallback: extract product names and create simple queries
                self.logger.warning(
                    "Failed to parse query generation JSON, using fallback"
                )
                return self._fallback_query_generation(question, conversation_history)

        except Exception as e:
            self.logger.error(f"Error generating search queries: {e}")
            return [question]  # Use original question as fallback

    def execute_intelligent_search(
        self,
        question: str,
        vector_results: List[Any],
        conversation_history: Optional[List[dict]] = None,
    ) -> Tuple[List[SearchResult], SearchDecision]:
        """Execute complete intelligent search process."""
        # Step 1: Decide search strategy
        decision = self.decide_search_strategy(
            question, vector_results, conversation_history
        )

        search_results = []

        if (
            decision.should_search
            and self.web_searcher
            and self.web_searcher.is_available()
        ):
            # Step 2: Generate search queries
            expected_info = getattr(
                decision, "expected_information", ["thông tin sản phẩm"]
            )
            queries = self.generate_search_queries(
                question, expected_info, conversation_history
            )

            # Step 3: Execute searches
            for query in queries[:2]:  # Limit to 2 queries to avoid overload
                try:
                    results = self.web_searcher.search_product_info(query)
                    search_results.extend(results)

                    if len(search_results) >= 5:  # Stop if we have enough results
                        break

                except Exception as e:
                    self.logger.warning(f"Search failed for query '{query}': {e}")
                    continue

            # Step 4: Deduplicate and rank results
            search_results = self._deduplicate_results(search_results)
            search_results = search_results[:5]  # Keep top 5 results

            self.logger.info(
                f"Executed intelligent search: {len(search_results)} results from {len(queries)} queries"
            )

        return search_results, decision

    def _summarize_vector_results(self, vector_results: List[Any]) -> str:
        """Summarize vector search results for LLM analysis."""
        if not vector_results:
            return "Không có kết quả từ vector search"

        summary_parts = []
        for i, doc in enumerate(vector_results[:3], 1):
            content_preview = (
                doc.page_content[:200] + "..."
                if len(doc.page_content) > 200
                else doc.page_content
            )
            summary_parts.append(f"Kết quả {i}: {content_preview}")

        return "\n".join(summary_parts)

    def _format_conversation_context(
        self, conversation_history: Optional[List[dict]]
    ) -> str:
        """Format conversation history for LLM context."""
        if not conversation_history:
            return "Không có lịch sử cuộc trò chuyện"

        recent_messages = (
            conversation_history[-2:]
            if len(conversation_history) > 2
            else conversation_history
        )

        context_parts = []
        for msg in recent_messages:
            user_msg = msg.get("message", "")
            if user_msg:
                context_parts.append(f"Người dùng: {user_msg}")

        return "\n".join(context_parts) if context_parts else "Không có ngữ cảnh"

    def _fallback_decision(
        self, question: str, vector_results: List[Any]
    ) -> SearchDecision:
        """Fallback rule-based decision when LLM fails."""
        # Simple rule-based logic
        should_search = (
            not vector_results
            or len(vector_results) < 2
            or any(
                keyword in question.lower()
                for keyword in ["giá", "so sánh", "mới", "hiện tại"]
            )
        )

        return SearchDecision(
            should_search=should_search,
            search_queries=[question],
            reasoning="Fallback rule-based decision",
            confidence=0.6,
            search_type="web" if should_search else "vector",
        )

    def _fallback_query_generation(
        self, question: str, conversation_history: Optional[List[dict]]
    ) -> List[str]:
        """Fallback query generation when LLM fails."""
        # Extract product names from conversation history
        product_names = set()

        if conversation_history:
            for msg in conversation_history[-2:]:
                text = (msg.get("message", "") + " " + msg.get("response", "")).lower()

                # Common product names
                products = [
                    "iphone",
                    "samsung",
                    "galaxy",
                    "xiaomi",
                    "oppo",
                    "vivo",
                    "realme",
                    "oneplus",
                    "huawei",
                    "nokia",
                    "asus",
                    "acer",
                    "dell",
                    "hp",
                    "lenovo",
                    "macbook",
                    "ipad",
                ]

                for product in products:
                    if product in text:
                        product_names.add(product)

        # Generate queries
        queries = [question]

        # Replace references with product names
        if product_names:
            for product in list(product_names)[:2]:  # Max 2 products
                modified_query = question.lower()
                references = [
                    "điện thoại trên",
                    "sản phẩm trên",
                    "thiết bị trên",
                    "máy trên",
                ]

                for ref in references:
                    modified_query = modified_query.replace(ref, product)

                if modified_query != question.lower():
                    queries.append(modified_query)

        return list(set(queries))  # Remove duplicates

    def _deduplicate_results(self, results: List[SearchResult]) -> List[SearchResult]:
        """Remove duplicate search results."""
        seen_urls = set()
        deduplicated = []

        for result in results:
            if result.href not in seen_urls:
                seen_urls.add(result.href)
                deduplicated.append(result)

        return sorted(deduplicated, key=lambda x: x.relevance_score, reverse=True)

    def get_search_explanation(self, decision: SearchDecision) -> Dict[str, Any]:
        """Get detailed explanation of search decision for debugging."""
        return {
            "decision": {
                "should_search": decision.should_search,
                "search_type": decision.search_type,
                "confidence": decision.confidence,
                "reasoning": decision.reasoning,
            },
            "queries_generated": len(decision.search_queries),
            "search_queries": decision.search_queries,
            "agent_type": "llm_driven",
            "timestamp": None,  # Could add timestamp if needed
        }
