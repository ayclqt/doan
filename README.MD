# ğŸ¤– Há»‡ Thá»‘ng Há»i ÄÃ¡p Sáº£n Pháº©m Äiá»‡n Tá»­ ThÃ´ng Minh

Má»™t há»‡ thá»‘ng chatbot AI tiÃªn tiáº¿n sá»­ dá»¥ng LangChain Agent, Vector Search vÃ  Web Search Ä‘á»ƒ tÆ° váº¥n sáº£n pháº©m Ä‘iá»‡n tá»­ má»™t cÃ¡ch thÃ´ng minh vÃ  chÃ­nh xÃ¡c.

## ğŸ“‹ Giá»›i Thiá»‡u Tá»•ng Quan

Há»‡ thá»‘ng Ä‘Æ°á»£c xÃ¢y dá»±ng vá»›i kiáº¿n trÃºc hiá»‡n Ä‘áº¡i, tÃ­ch há»£p AI Agent thÃ´ng minh cÃ³ kháº£ nÄƒng:

- ğŸ§  **AI Agent System**: Sá»­ dá»¥ng 6 tools chuyÃªn biá»‡t Ä‘á»ƒ xá»­ lÃ½ cÃ¢u há»i phá»©c táº¡p
- ğŸ” **Vector Search**: TÃ¬m kiáº¿m thÃ´ng tin sáº£n pháº©m tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u Qdrant
- ğŸŒ **Web Search**: TÃ¬m kiáº¿m thÃ´ng tin má»›i nháº¥t trÃªn internet
- ğŸ’¬ **Streaming Response**: Pháº£n há»“i real-time vá»›i Server-Sent Events
- ğŸ” **Authentication**: Báº£o máº­t vá»›i JWT vÃ  Redis session
- ğŸ“Š **Performance Monitoring**: Theo dÃµi hiá»‡u suáº¥t vÃ  debug chi tiáº¿t

## ğŸ—ï¸ Kiáº¿n TrÃºc Há»‡ Thá»‘ng

### SÆ¡ Äá»“ Tá»•ng Quan

```mermaid
graph TB
    subgraph "ğŸ–¥ï¸ Frontend Layer"
        ST[Streamlit UI<br/>Port: 8501]
        API_CLIENT[API Client<br/>aiohttp]
    end
    
    subgraph "ğŸš€ Backend API Layer"
        LITESTAR[Litestar API Server<br/>Port: 8000]
        AUTH[Authentication Service<br/>JWT + BCrypt]
        CHAT[Chat Controller<br/>Streaming Support]
        CONV[Conversation Service<br/>Redis Storage]
    end
    
    subgraph "ğŸ¤– AI Processing Layer"
        AGENT[AI Agent System<br/>6 Intelligent Tools]
        PIPELINE[LangChain Pipeline<br/>Context Management]
        VECTOR[Vector Search<br/>Vietnamese Embedding]
        WEB[Web Search<br/>DuckDuckGo API]
    end
    
    subgraph "ğŸ’¾ Data Layer"
        QDRANT[(Qdrant Vector DB<br/>gRPC: 6334 (only protocol))]
        REDIS[(Redis Cache<br/>Unix Socket: /tmp/redis.sock)]
    end
    
    ST --> API_CLIENT
    API_CLIENT --> LITESTAR
    LITESTAR --> AUTH
    LITESTAR --> CHAT
    CHAT --> CONV
    CHAT --> AGENT
    AGENT --> PIPELINE
    PIPELINE --> VECTOR
    PIPELINE --> WEB
    VECTOR --> QDRANT
    CONV --> REDIS
    AUTH --> REDIS
    
    style AGENT fill:#e1f5fe
    style PIPELINE fill:#f3e5f5
    style VECTOR fill:#e8f5e8
    style WEB fill:#fff3e0
```

### Luá»“ng Xá»­ LÃ½ Chat

```mermaid
sequenceDiagram
    participant U as ğŸ‘¤ User
    participant ST as ğŸ–¥ï¸ Streamlit
    participant API as ğŸš€ Litestar API
    participant AGENT as ğŸ¤– AI Agent
    participant VS as ğŸ” Vector Store
    participant WS as ğŸŒ Web Search
    participant DB as ğŸ’¾ Databases
    
    U->>ST: Nháº­p cÃ¢u há»i sáº£n pháº©m
    ST->>API: POST /chat (stream=true)
    
    Note over API: XÃ¡c thá»±c JWT Token
    API->>AGENT: PhÃ¢n tÃ­ch cÃ¢u há»i
    
    Note over AGENT: Quyáº¿t Ä‘á»‹nh tools cáº§n dÃ¹ng
    AGENT->>VS: vector_search_tool
    VS-->>AGENT: ThÃ´ng tin sáº£n pháº©m tá»« DB
    
    AGENT->>WS: web_search_tool (náº¿u cáº§n)
    WS-->>AGENT: ThÃ´ng tin giÃ¡ cáº£ má»›i nháº¥t
    
    AGENT->>API: Táº¡o cÃ¢u tráº£ lá»i
    API->>ST: Server-Sent Events Stream
    ST->>U: Hiá»ƒn thá»‹ real-time response
    
    Note over API: LÆ°u conversation
    API->>DB: Redis + Qdrant storage
```

### AI Agent Tools System

```mermaid
graph LR
    subgraph "ğŸ¤– AI Agent Core"
        AGENT[Product Assistant Agent]
    end
    
    subgraph "ğŸ› ï¸ Available Tools"
        T1[vector_search<br/>ğŸ“Š Product Database]
        T2[web_search<br/>ğŸŒ Current Info]
        T3[resolve_reference<br/>ğŸ’­ Context Memory]
        T4[compare_products<br/>âš–ï¸ Product Comparison]
        T5[analyze_price<br/>ğŸ’° Price Analysis]
        T6[analyze_context<br/>ğŸ§  Intent Analysis]
    end
    
    AGENT --> T1
    AGENT --> T2
    AGENT --> T3
    AGENT --> T4
    AGENT --> T5
    AGENT --> T6
    
    T1 --> QDRANT[(Qdrant<br/>Vector DB)]
    T2 --> DUCK[DuckDuckGo<br/>Search API]
    T3 --> CONV[Conversation<br/>History]
    
    style AGENT fill:#e3f2fd
    style T1 fill:#e8f5e8
    style T2 fill:#fff3e0
    style T3 fill:#f3e5f5
    style T4 fill:#e0f2f1
    style T5 fill:#fce4ec
    style T6 fill:#e1f5fe
```

## âœ¨ TÃ­nh NÄƒng ChÃ­nh

### ğŸ¤– AI Agent System
- **6 Tools ThÃ´ng Minh**: Tá»± Ä‘á»™ng chá»n tool phÃ¹ há»£p cho tá»«ng cÃ¢u há»i
- **Context Awareness**: Hiá»ƒu ngá»¯ cáº£nh cuá»™c trÃ² chuyá»‡n, resolve tham chiáº¿u
- **Tool Chaining**: Káº¿t há»£p nhiá»u tools Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i phá»©c táº¡p
- **Performance Tracking**: Theo dÃµi hiá»‡u suáº¥t vÃ  tá»‘i Æ°u hÃ³a

### ğŸ” Vector Search
- **Vietnamese Embedding**: AITeamVN/Vietnamese_Embedding model
- **Qdrant Integration**: High-performance vector database
- **Semantic Search**: TÃ¬m kiáº¿m theo ngá»¯ nghÄ©a, khÃ´ng chá»‰ tá»« khÃ³a
- **Chunking Strategy**: Chunk size 1000, overlap 200 tokens

### ğŸŒ Web Search
- **Real-time Information**: ThÃ´ng tin giÃ¡ cáº£, khuyáº¿n mÃ£i má»›i nháº¥t
- **Smart Decision**: AI quyáº¿t Ä‘á»‹nh khi nÃ o cáº§n web search
- **DuckDuckGo Integration**: Privacy-focused search engine
- **Regional Search**: TÃ¬m kiáº¿m tá»‘i Æ°u cho thá»‹ trÆ°á»ng Viá»‡t Nam

### ğŸ’¬ Streaming Response
- **Real-time Chat**: Server-Sent Events streaming
- **Progress Tracking**: Hiá»ƒn thá»‹ tiáº¿n trÃ¬nh xá»­ lÃ½
- **Error Handling**: Xá»­ lÃ½ lá»—i graceful
- **Background Saving**: LÆ°u conversation khÃ´ng blocking

### ğŸ” Security & Authentication
- **JWT Authentication**: Secure token-based auth
- **BCrypt Hashing**: Password encryption
- **Redis Sessions**: Distributed session management
- **Rate Limiting**: API protection
- **CORS Support**: Cross-origin resource sharing

### ğŸ“Š Conversation Management
- **Redis Storage**: Fast conversation retrieval
- **Pagination**: Efficient conversation listing
- **Auto-titling**: AI-generated conversation titles
- **History Search**: Find past conversations
- **Export/Import**: Conversation backup support

## ğŸš€ CÃ i Äáº·t vÃ  Cháº¡y

### YÃªu Cáº§u Há»‡ Thá»‘ng
- Python 3.12+
- Docker & Docker Compose
- 4GB RAM minimum (8GB recommended)
- 10GB disk space

### 1. Clone Repository
```bash
git clone <repository-url>
cd DoAn
```

### 2. Cáº¥u HÃ¬nh Environment
```bash
cp .env.example .env
```

Chá»‰nh sá»­a file `.env`:
```env
# LLM Configuration
OPENAI_API_KEY=your_api_key_here
OPENAI_BASE_URL=https://api.x.ai/v1
LLM_MODEL_NAME=grok-3-mini

# Database Configuration
QDRANT_URL=localhost
QDRANT_PORT=6334                    # gRPC API port (only protocol supported)
REDIS_URL=unix:///tmp/redis.sock

# Embedding Model
EMBEDDING_MODEL_NAME=AITeamVN/Vietnamese_Embedding

# API Configuration
API_USER=admin
API_PASS=admin
JWT_SECRET=your_secret_here
```

> **LÆ°u Ã½ Redis Configuration:**
> - Há»‡ thá»‘ng sá»­ dá»¥ng Redis qua Unix socket Ä‘á»ƒ tá»‘i Æ°u hiá»‡u suáº¥t
> - Náº¿u cáº§n fallback TCP: `REDIS_URL=redis://localhost:6379/`
> - Unix socket cÃ³ latency tháº¥p hÆ¡n vÃ  báº£o máº­t tá»‘t hÆ¡n TCP

### 3. Cháº¡y vá»›i Docker Compose
```bash
# Khá»Ÿi Ä‘á»™ng databases
docker-compose up -d qdrant redis

# CÃ i Ä‘áº·t dependencies
pip install -e .

# Khá»Ÿi táº¡o dá»¯ liá»‡u
python ingest.py
python init_admin_user.py
python init_conversation_collections.py
```

## ğŸ”§ Qdrant gRPC Configuration chi tiáº¿t

### gRPC vs HTTP Performance

```mermaid
graph LR
    subgraph "ğŸš€ gRPC Connection (Recommended)"
        APP1[Application] -->|Binary Protocol| GRPC[gRPC Port 6334]
        GRPC --> QDRANT1[Qdrant Process]
    end
    
    subgraph "ğŸŒ HTTP REST Connection"  
        APP2[Application] -->|JSON/HTTP| HTTP[HTTP Port 6333]
        HTTP --> QDRANT2[Qdrant Process]
    end
    
    style APP1 fill:#e8f5e8
    style GRPC fill:#e8f5e8
    style QDRANT1 fill:#e8f5e8
    style APP2 fill:#fff3e0
    style HTTP fill:#fff3e0
    style QDRANT2 fill:#fff3e0
```

### Lá»£i Ã­ch gRPC
- **Hiá»‡u suáº¥t cao hÆ¡n**: Binary protocol, nhanh hÆ¡n 30-50% so vá»›i HTTP
- **Bandwidth efficiency**: Ãt dá»¯ liá»‡u truyá»n táº£i hÆ¡n
- **Type Safety**: Strongly typed vá»›i Protocol Buffers
- **Streaming Support**: Native support cho streaming operations
- **Connection Multiplexing**: Má»™t connection cho multiple requests

### Docker Compose Qdrant Setup

```mermaid
graph TB
    subgraph "ğŸ³ Qdrant Container"
        QDRANT_SERVICE[Qdrant Server<br/>QDRANT__SERVICE__GRPC_PORT=6334<br/>QDRANT__SERVICE__HTTP_PORT=6333]
        GRPC_PORT[gRPC Port 6334<br/>Binary Protocol]
        HTTP_PORT[HTTP Port 6333<br/>REST API Fallback]
        STORAGE[Vector Storage<br/>/qdrant/storage]
    end
    
    subgraph "Application Layer"
        VECTORSTORE[VectorStore Class<br/>Smart Connection Management]
        FALLBACK[Automatic Fallback<br/>gRPC â†’ HTTP]
    end
    
    QDRANT_SERVICE --> GRPC_PORT
    QDRANT_SERVICE --> HTTP_PORT
    QDRANT_SERVICE --> STORAGE
    
    VECTORSTORE -->|Preferred| GRPC_PORT
    VECTORSTORE -->|Fallback| HTTP_PORT
    FALLBACK --> VECTORSTORE
    
    style QDRANT_SERVICE fill:#e1f5fe
    style GRPC_PORT fill:#e8f5e8
    style HTTP_PORT fill:#fff3e0
    style VECTORSTORE fill:#e8f5e8
    style FALLBACK fill:#f3e5f5
```

### Connection Strategy
```python
# Automatic connection vá»›i fallback
if config.qdrant_use_grpc:
    try:
        # Thá»­ gRPC trÆ°á»›c (preferred)
        client = QdrantClient(
            url=url,
            port=grpc_port,
            grpc_port=grpc_port,
            prefer_grpc=True
        )
    except Exception:
        # Fallback sang HTTP náº¿u gRPC fail
        client = QdrantClient(url=url, port=http_port)
```

### Performance Benchmarking

```bash
# Benchmark gRPC connection
python -c "
from src.langchain_integration.vectorstore import VectorStore
import time

# Test gRPC performance
start = time.time()
vs = VectorStore(use_grpc=True)
vs.similarity_search('iPhone 15', k=10)
grpc_time = time.time() - start

print(f'gRPC search time: {grpc_time:.3f}s')
"

# Benchmark HTTP connection
python -c "
from src.langchain_integration.vectorstore import VectorStore
import time

# Test HTTP performance
start = time.time()
vs = VectorStore(use_grpc=False)
vs.similarity_search('iPhone 15', k=10)
http_time = time.time() - start

print(f'HTTP search time: {http_time:.3f}s')
"
```

### Qdrant Connection Troubleshooting

```bash
# Kiá»ƒm tra Qdrant container status
docker ps | grep qdrant

# Kiá»ƒm tra gRPC port accessibility
telnet localhost 6334

# Kiá»ƒm tra HTTP port accessibility  
curl http://localhost:6333/collections

# Xem Qdrant logs
docker-compose logs qdrant

# Test connection tá»« Python
python -c "
from src.langchain_integration.vectorstore import VectorStore
vs = VectorStore()
print('Connection info:', vs.get_connection_info())
"
```

### Configuration Options

```env
# gRPC Configuration (Recommended)
QDRANT_USE_GRPC=true
QDRANT_GRPC_PORT=6334

# HTTP Fallback
QDRANT_PORT=6333

# Connection timeout
QDRANT_TIMEOUT=10.0

# Debugging
QDRANT_DEBUG=false
```

### Common Issues & Solutions

#### âŒ gRPC connection failed
```bash
# Kiá»ƒm tra port binding
docker port <qdrant-container> 6334

# Restart Qdrant service
docker-compose restart qdrant

# Fallback sang HTTP temporarily
export QDRANT_USE_GRPC=false
```

#### âŒ Collection khÃ´ng tá»“n táº¡i
```bash
# Táº¡o collection má»›i
python init_conversation_collections.py

# Kiá»ƒm tra collections
curl http://localhost:6333/collections
```

#### âŒ Embedding dimension mismatch
```bash
# Recreate collection vá»›i Ä‘Ãºng vector size
python -c "
from src.langchain_integration.vectorstore import VectorStore
vs = VectorStore()
vs.create_collection(vector_size=768)  # Adjust size
"
```

### Development Tips

```bash
# Monitor gRPC connection health
python -c "
from src.langchain_integration.vectorstore import VectorStore
vs = VectorStore()
info = vs.get_connection_info()
print(f'Connected: {info[\"connected\"]}')
print(f'Protocol: {\"gRPC\" if info[\"use_grpc\"] else \"HTTP\"}')
print(f'Collections: {info[\"collections_count\"]}')
"

# Performance comparison
python -c "
import time
from src.langchain_integration.vectorstore import VectorStore

# Test both protocols
for use_grpc in [True, False]:
    vs = VectorStore(use_grpc=use_grpc)
    start = time.time()
    vs.similarity_search('test query', k=5)
    elapsed = time.time() - start
    protocol = 'gRPC' if use_grpc else 'HTTP'
    print(f'{protocol}: {elapsed:.3f}s')
"
```

## ğŸ”§ Redis Configuration chi tiáº¿t

### Unix Socket vs TCP Performance

```mermaid
graph LR
    subgraph "ğŸš€ Unix Socket (Recommended)"
        APP1[Application] -->|IPC| SOCKET[/tmp/redis.sock]
        SOCKET --> REDIS1[Redis Process]
    end
    
    subgraph "ğŸŒ TCP Connection"  
        APP2[Application] -->|TCP/IP| PORT[localhost:6379]
        PORT --> REDIS2[Redis Process]
    end
    
    style APP1 fill:#e8f5e8
    style SOCKET fill:#e8f5e8
    style REDIS1 fill:#e8f5e8
    style APP2 fill:#fff3e0
    style PORT fill:#fff3e0
    style REDIS2 fill:#fff3e0
```

### Docker Compose Redis Setup

```mermaid
graph TB
    subgraph "ğŸ³ Docker Services"
        subgraph "Redis Container"
            REDIS_SERVICE[Redis Server<br/>--unixsocket /tmp/redis.sock<br/>--unixsocketperm 755]
            REDIS_SOCKET[/tmp/redis.sock]
            REDIS_PORT[Port 6379<br/>TCP Fallback]
        end
        
        subgraph "Application Container"
            APP[Application<br/>REDIS_URL=unix:///tmp/redis.sock]
        end
        
        subgraph "Shared Volumes"
            SOCKET_VOLUME[redis_socket:/tmp]
            DATA_VOLUME[redis_data:/data]
        end
    end
    
    REDIS_SERVICE --> REDIS_SOCKET
    REDIS_SERVICE --> REDIS_PORT
    REDIS_SOCKET --> SOCKET_VOLUME
    SOCKET_VOLUME --> APP
    DATA_VOLUME --> REDIS_SERVICE
    
    style REDIS_SERVICE fill:#e1f5fe
    style REDIS_SOCKET fill:#e8f5e8
    style APP fill:#e8f5e8
    style SOCKET_VOLUME fill:#f3e5f5
```

### Lá»£i Ã­ch Unix Socket
- **Hiá»‡u suáº¥t cao hÆ¡n**: KhÃ´ng cáº§n network stack overhead
- **Latency tháº¥p hÆ¡n**: Direct Inter-Process Communication (IPC)
- **Báº£o máº­t tá»‘t hÆ¡n**: Chá»‰ accessible tá»« local machine
- **Resource efficiency**: Ãt CPU vÃ  memory overhead

### Connection Strings
```env
# Recommended: Unix Socket
REDIS_URL=unix:///tmp/redis.sock

# Fallback: TCP (cho development/testing)
REDIS_URL=redis://localhost:6379/

# Vá»›i database index
REDIS_URL=unix:///tmp/redis.sock?db=0

# Vá»›i password (náº¿u cáº§n)
REDIS_URL=unix://:password@/tmp/redis.sock
```

## ğŸ› ï¸ Redis Troubleshooting

### Kiá»ƒm tra Redis Connection

```bash
# Kiá»ƒm tra Redis container Ä‘ang cháº¡y
docker ps | grep redis

# Kiá»ƒm tra Redis socket file
docker exec redis-container ls -la /tmp/redis.sock

# Test connection vá»›i redis-cli
docker exec redis-container redis-cli -s /tmp/redis.sock ping

# Test tá»« application
python -c "
import redis
r = redis.from_url('unix:///tmp/redis.sock')
print('Redis connection:', r.ping())
"
```

### Common Issues & Solutions

#### âŒ Socket file khÃ´ng tá»“n táº¡i
```bash
# Kiá»ƒm tra volume mount
docker volume inspect <project>_redis_socket

# Khá»Ÿi Ä‘á»™ng láº¡i Redis container
docker-compose restart redis
```

#### âŒ Permission denied
```bash
# Kiá»ƒm tra socket permissions
docker exec redis-container ls -la /tmp/redis.sock

# Náº¿u cáº§n, adjust permissions
docker exec redis-container chmod 755 /tmp/redis.sock
```

#### âŒ Connection refused
```bash
# Kiá»ƒm tra Redis process
docker exec redis-container ps aux | grep redis

# Kiá»ƒm tra Redis logs
docker-compose logs redis

# Fallback sang TCP
export REDIS_URL="redis://localhost:6379/"
```

### Development Tips

```bash
# Cháº¡y Redis cli trong container
docker exec -it redis-container redis-cli -s /tmp/redis.sock

# Monitor Redis commands
docker exec redis-container redis-cli -s /tmp/redis.sock monitor

# Kiá»ƒm tra Redis info
docker exec redis-container redis-cli -s /tmp/redis.sock info

# Backup/restore qua Unix socket
docker exec redis-container redis-cli -s /tmp/redis.sock --rdb /tmp/dump.rdb
```

### Performance Benchmarking

```bash
# Benchmark Unix socket
docker exec redis-container redis-cli -s /tmp/redis.sock --latency-history

# So sÃ¡nh vá»›i TCP
docker exec redis-container redis-cli -h localhost -p 6379 --latency-history

# Throughput test
docker exec redis-container redis-cli -s /tmp/redis.sock eval "
for i=1,10000 do 
  redis.call('set', 'bench:' .. i, 'value' .. i) 
end
" 0
```

### 4. Khá»Ÿi Äá»™ng Services

**Backend API:**
```bash
litestar run
# Hoáº·c: python -m litestar run
# API server: http://localhost:8000
```

**Frontend UI:**
```bash
streamlit run src/streamlit/app.py
# UI: http://localhost:8501
```

### 5. Development Mode
```bash
# Install development dependencies
pip install -e ".[dev]"

# Run with auto-reload
litestar run --reload

# Run tests
pytest

# Code formatting
ruff format .
ruff check .
```

## ğŸ“š API Documentation

### Authentication
```bash
# Login
curl -X POST http://localhost:8000/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username": "admin", "password": "admin"}'

# Register
curl -X POST http://localhost:8000/auth/register \
  -H "Content-Type: application/json" \
  -d '{"username": "newuser", "password": "password123"}'
```

### Chat Endpoints
```bash
# Non-streaming chat
curl -X POST http://localhost:8000/chat/ \
  -H "Authorization: Bearer <token>" \
  -H "Content-Type: application/json" \
  -d '{"message": "TÆ° váº¥n laptop gaming táº§m giÃ¡ 20 triá»‡u", "stream": false}'

# Streaming chat
curl -X POST http://localhost:8000/chat/ \
  -H "Authorization: Bearer <token>" \
  -H "Content-Type: application/json" \
  -d '{"message": "So sÃ¡nh iPhone 15 vs Samsung S24", "stream": true}'
```

### Conversation Management
```bash
# List conversations
curl -X GET http://localhost:8000/chat/conversations \
  -H "Authorization: Bearer <token>"

# Get conversation history
curl -X GET http://localhost:8000/chat/conversations/<conversation_id> \
  -H "Authorization: Bearer <token>"
```

### Monitoring Endpoints
```bash
# Agent statistics
curl -X GET http://localhost:8000/chat/agent-stats \
  -H "Authorization: Bearer <token>"

# Search debug info
curl -X GET "http://localhost:8000/chat/search-info?query=iPhone 15" \
  -H "Authorization: Bearer <token>"

# Health check
curl -X GET http://localhost:8000/health
```

### Streaming Format
Server-Sent Events format:
```
data: {"type": "start", "conversation_id": "uuid", "content": ""}

data: {"type": "chunk", "content": "Xin chÃ o! TÃ´i cÃ³ thá»ƒ", "conversation_id": "uuid"}

data: {"type": "chunk", "content": " giÃºp báº¡n tÆ° váº¥n", "conversation_id": "uuid"}

data: {"type": "end", "content": "", "metadata": {"total_length": 150}}
```

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
DoAn/
â”œâ”€â”€ ğŸš€ app.py                          # Main Litestar application
â”œâ”€â”€ ğŸ“‹ pyproject.toml                  # Python project configuration
â”œâ”€â”€ ğŸ³ compose.yaml                    # Docker Compose services
â”œâ”€â”€ ğŸ”§ *.py                           # Utility scripts (ingest, init, etc.)
â”‚
â”œâ”€â”€ ğŸ“‚ src/                           # Source code chÃ­nh
â”‚   â”œâ”€â”€ ğŸ”§ config/                    # Configuration management
â”‚   â”‚   â””â”€â”€ config.py                 # Settings vÃ  environment variables
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸŒ api/                       # REST API layer
â”‚   â”‚   â”œâ”€â”€ ğŸ” auth/                  # Authentication services
â”‚   â”‚   â”œâ”€â”€ ğŸ›¡ï¸ middleware/            # API middleware (CORS, logging, rate limit)
â”‚   â”‚   â”œâ”€â”€ ğŸ›£ï¸ routes/                # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py               # Auth routes
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py               # Chat routes (main functionality)
â”‚   â”‚   â”‚   â””â”€â”€ health.py             # Health check
â”‚   â”‚   â”œâ”€â”€ ğŸ“‹ schemas/               # Pydantic models
â”‚   â”‚   â””â”€â”€ ğŸ”§ services/              # Business logic services
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ¤– langchain_integration/     # AI & LangChain components
â”‚   â”‚   â”œâ”€â”€ agent_system.py           # ğŸ§  AI Agent vá»›i 6 tools
â”‚   â”‚   â”œâ”€â”€ pipeline.py               # ğŸ”„ LangChain processing pipeline
â”‚   â”‚   â”œâ”€â”€ vectorstore.py            # ğŸ” Qdrant vector database
â”‚   â”‚   â”œâ”€â”€ web_search.py             # ğŸŒ DuckDuckGo web search
â”‚   â”‚   â”œâ”€â”€ llm_search_system.py      # ğŸ¯ LLM decision system
â”‚   â”‚   â”œâ”€â”€ llm_search_agent.py       # ğŸ¤– LLM search agent
â”‚   â”‚   â”œâ”€â”€ product_introduction_agent.py # ğŸ“± Product intro agent
â”‚   â”‚   â””â”€â”€ text_processor.py         # ğŸ“ Text processing utilities
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ–¥ï¸ streamlit/                 # Frontend UI
â”‚       â”œâ”€â”€ app.py                    # Main Streamlit application
â”‚       â””â”€â”€ README.md                 # Streamlit-specific docs
â”‚
â”œâ”€â”€ ğŸ“š docs/                          # Documentation
â”‚   â”œâ”€â”€ ğŸ—ï¸ arch.mmd                  # Architecture diagrams
â”‚   â”œâ”€â”€ ğŸ“– *.md                       # Various documentation files
â”‚   â””â”€â”€ ğŸ“Š *.mmd                      # Mermaid diagrams
â”‚
â””â”€â”€ ğŸ““ notebooks/                     # Jupyter notebooks cho development
    â””â”€â”€ qdrant_import_export.ipynb    # Database utilities
```

### Key Components

- **ğŸ¤– Agent System**: TrÃ¡i tim cá»§a há»‡ thá»‘ng, xá»­ lÃ½ logic AI
- **ğŸ” Vector Store**: TÃ¬m kiáº¿m semantic vá»›i Qdrant
- **ğŸŒ Web Search**: ThÃ´ng tin real-time tá»« internet
- **ğŸ’¬ Chat Pipeline**: Xá»­ lÃ½ conversation vÃ  streaming
- **ğŸ” Auth System**: Báº£o máº­t vÃ  quáº£n lÃ½ user
- **ğŸ–¥ï¸ Streamlit UI**: Giao diá»‡n user-friendly

## ğŸ“Š Monitoring vÃ  Debug

### Agent Performance Statistics
```bash
# Xem thá»‘ng kÃª agent
GET /chat/agent-stats
```

Response:
```json
{
  "agent_enabled": true,
  "system_stats": {
    "performance": {
      "total_queries": 150,
      "tool_calls": 287,
      "successful_resolutions": 148,
      "average_tools_per_query": 1.91
    },
    "available_tools": ["vector_search", "web_search", "resolve_conversation_reference", ...]
  }
}
```

### Search Debug Information
```bash
# Debug search decisions
GET /chat/search-info?query=iPhone 15 Pro Max
```

Response:
```json
{
  "vector_results_count": 5,
  "vector_results": [...],
  "web_search_enabled": true,
  "would_use_web_search": true,
  "web_results_count": 3,
  "web_results": [...],
  "agent_reasoning": ["Step 1: Used vector_search", "Step 2: Used web_search"]
}
```

### Performance Metrics
- **Tool Usage**: Theo dÃµi frequency má»—i tool Ä‘Æ°á»£c sá»­ dá»¥ng
- **Response Time**: Average processing time cho má»—i query
- **Cache Hit Rate**: Tá»· lá»‡ cache hit cá»§a decision system
- **Error Rate**: Tracking lá»—i vÃ  failure cases

### System Optimization
```bash
# Reset agent statistics
POST /chat/agent-reset

# Optimize system performance
# (Automatically clears cache, resets counters)
```

### Logging & Monitoring
- **Structured Logging**: Sá»­ dá»¥ng structlog cho consistent logging
- **Request Tracing**: Track má»—i request tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i
- **Performance Profiling**: Monitor database query times
- **Health Checks**: Automated service health monitoring

---

## ğŸ‘¥ TÃ¡c Giáº£

**LÃ¢m Quang TrÃ­**
- Email: quangtri.lam.9@gmail.com
- GitHub: [Profile]

## ğŸ“„ License

MIT License - xem file [LICENSE](LICENSE) Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

---

## ğŸ¯ Roadmap

- [ ] ğŸŒ Multi-language support
- [ ] ğŸ“± Mobile app development
- [ ] ğŸ”Š Voice interaction
- [ ] ğŸ“ˆ Advanced analytics dashboard
- [ ] ğŸ¤– More specialized agents
- [ ] ğŸ”— Third-party integrations

---

*Há»‡ thá»‘ng Ä‘Æ°á»£c phÃ¡t triá»ƒn vá»›i â¤ï¸ sá»­ dá»¥ng Python, LangChain, vÃ  cÃ¡c cÃ´ng nghá»‡ AI hiá»‡n Ä‘áº¡i*
